"""
PDF Proxy Service for Orchestrate OS

A lightweight FastAPI proxy that fetches PDFs from allowlisted hosts and serves
them with proper headers to avoid CORS issues and download prompts.

Usage:
    uvicorn pdf_proxy:app --host 0.0.0.0 --port 8000

Environment Variables:
    PDF_PROXY_ALLOWED_HOSTS: Comma-separated list of allowed hostnames
        Default: app-myautobots-public-dev.s3.amazonaws.com
    PDF_PROXY_ALLOWED_ORIGINS: CORS allowed origins (comma-separated)
        Default: * (all origins)
    PDF_PROXY_MAX_SIZE_MB: Maximum file size in MB
        Default: 25
"""

import os
import ipaddress
import socket
from urllib.parse import urlparse, unquote
from typing import Optional

from pathlib import Path
from fastapi import FastAPI, Query, HTTPException, Response
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
import httpx
import fitz  # type: ignore[import-untyped]  # PyMuPDF

app = FastAPI(
    title="Orchestrate OS PDF Proxy",
    description="CORS-safe PDF proxy for Record Inspection with static file serving",
    version="1.1.0"
)

from server.db import init_pool, close_pool, check_health
from server.migrate import run_migrations
from server.api_v25 import router as api_v25_router
from server.routes.workspaces import router as workspaces_router
from server.routes.batches import router as batches_router
from server.routes.patches import router as patches_router
from server.routes.contracts import router as contracts_router
from server.routes.documents import router as documents_router
from server.routes.accounts import router as accounts_router
from server.routes.annotations import router as annotations_router
from server.routes.evidence_packs import router as evidence_packs_router
from server.routes.rfis import router as rfis_router
from server.routes.triage_items import router as triage_items_router
from server.routes.signals import router as signals_router
from server.routes.selection_captures import router as selection_captures_router
from server.routes.audit_events import router as audit_events_router
from server.routes.sse_stream import router as sse_router
from server.routes.auth_google import router as auth_google_router
from server.routes.members import router as members_router
from server.routes.drive import router as drive_router
from server.routes.sessions import router as sessions_router
import logging as _logging

@app.on_event("startup")
def _startup_v25():
    _log = _logging.getLogger("server.startup")
    _logging.basicConfig(level=_logging.INFO, format="%(levelname)s: %(message)s")
    try:
        run_migrations()
    except Exception as e:
        _log.warning("Migration failed (DB may not be ready): %s", e)
    try:
        init_pool()
    except Exception as e:
        _log.warning("DB pool init failed: %s", e)
        return
    if check_health():
        _log.info("DB connection verified (SELECT 1 OK)")
    else:
        _log.error("DB connection verification FAILED (SELECT 1)")

@app.on_event("shutdown")
def _shutdown_v25():
    close_pool()

app.include_router(api_v25_router)
app.include_router(workspaces_router)
app.include_router(batches_router)
app.include_router(patches_router)
app.include_router(contracts_router)
app.include_router(documents_router)
app.include_router(accounts_router)
app.include_router(annotations_router)
app.include_router(evidence_packs_router)
app.include_router(rfis_router)
app.include_router(triage_items_router)
app.include_router(signals_router)
app.include_router(selection_captures_router)
app.include_router(audit_events_router)
app.include_router(sse_router)
app.include_router(auth_google_router)
app.include_router(members_router)
app.include_router(drive_router)
app.include_router(sessions_router)

PROJECT_ROOT = Path(__file__).parent.parent

DEFAULT_ALLOWED_HOSTS = [
    "app-myautobots-public-dev.s3.amazonaws.com",
    "s3.amazonaws.com",
    "s3.us-east-1.amazonaws.com",
    "s3.us-west-2.amazonaws.com",
]

ALLOWED_HOSTS = os.environ.get(
    "PDF_PROXY_ALLOWED_HOSTS",
    ",".join(DEFAULT_ALLOWED_HOSTS)
).split(",")
ALLOWED_HOSTS = [h.strip().lower() for h in ALLOWED_HOSTS if h.strip()]

ALLOWED_ORIGINS = os.environ.get("PDF_PROXY_ALLOWED_ORIGINS", "*").split(",")
ALLOWED_ORIGINS = [o.strip() for o in ALLOWED_ORIGINS if o.strip()]

MAX_SIZE_MB = int(os.environ.get("PDF_PROXY_MAX_SIZE_MB", "25"))
MAX_SIZE_BYTES = MAX_SIZE_MB * 1024 * 1024

app.add_middleware(
    CORSMiddleware,
    allow_origins=ALLOWED_ORIGINS,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
    expose_headers=["Content-Disposition", "Content-Type", "X-Proxy-Source"],
)


def is_private_ip(hostname: str) -> bool:
    """Check if hostname resolves to a private IP (SSRF guard)."""
    try:
        ip = socket.gethostbyname(hostname)
        ip_obj = ipaddress.ip_address(ip)
        return ip_obj.is_private or ip_obj.is_loopback or ip_obj.is_reserved
    except (socket.gaierror, ValueError):
        return True


def is_host_allowed(hostname: str) -> bool:
    """Check if hostname is in the allowlist."""
    hostname_lower = hostname.lower()
    for allowed in ALLOWED_HOSTS:
        if hostname_lower == allowed or hostname_lower.endswith("." + allowed):
            return True
    return False


@app.get("/health")
async def health_check():
    """Health check endpoint."""
    return {"status": "ok", "allowed_hosts": ALLOWED_HOSTS}


@app.get("/proxy/pdf")
async def proxy_pdf(url: str = Query(..., description="URL of the PDF to fetch")):
    """
    Fetch a PDF from an allowlisted host and return it with inline disposition.
    
    Security:
    - Only allowlisted hosts are permitted
    - Private IPs are blocked (SSRF guard)
    - Size limit enforced via Content-Length
    """
    try:
        decoded_url = unquote(url)
        parsed = urlparse(decoded_url)
    except Exception:
        raise HTTPException(status_code=400, detail="Invalid URL format")
    
    if parsed.scheme not in ("http", "https"):
        raise HTTPException(status_code=400, detail="Only HTTP/HTTPS URLs allowed")
    
    hostname = parsed.hostname
    if not hostname:
        raise HTTPException(status_code=400, detail="Missing hostname in URL")
    
    if not is_host_allowed(hostname):
        raise HTTPException(
            status_code=403,
            detail=f"Host not in allowlist: {hostname}"
        )
    
    if is_private_ip(hostname):
        raise HTTPException(
            status_code=403,
            detail="Private/reserved IPs are blocked"
        )
    
    # v1.4.16: Disable redirects and validate final URL for SSRF protection
    async with httpx.AsyncClient(timeout=30.0, follow_redirects=False) as client:
        try:
            # First check size via HEAD (if available)
            try:
                head_resp = await client.head(decoded_url)
                content_length = head_resp.headers.get("content-length")
                
                if content_length:
                    size = int(content_length)
                    if size > MAX_SIZE_BYTES:
                        raise HTTPException(
                            status_code=413,
                            detail=f"File too large: {size} bytes (max {MAX_SIZE_BYTES})"
                        )
                
                # Handle redirects manually with SSRF validation
                if head_resp.status_code in (301, 302, 303, 307, 308):
                    redirect_url = head_resp.headers.get("location")
                    if redirect_url:
                        redirect_parsed = urlparse(redirect_url)
                        redirect_host = redirect_parsed.hostname
                        if not redirect_host or not is_host_allowed(redirect_host) or is_private_ip(redirect_host):
                            raise HTTPException(status_code=403, detail="Redirect to non-allowlisted host blocked")
                        decoded_url = redirect_url
            except httpx.HTTPStatusError:
                pass  # Some servers don't support HEAD
            
            # Stream response with size enforcement
            resp = await client.get(decoded_url)
            
            # Handle redirects on GET as well
            if resp.status_code in (301, 302, 303, 307, 308):
                redirect_url = resp.headers.get("location")
                if redirect_url:
                    redirect_parsed = urlparse(redirect_url)
                    redirect_host = redirect_parsed.hostname
                    if not redirect_host or not is_host_allowed(redirect_host) or is_private_ip(redirect_host):
                        raise HTTPException(status_code=403, detail="Redirect to non-allowlisted host blocked")
                    resp = await client.get(redirect_url)
            
            resp.raise_for_status()
            
            # Enforce size limit on actual content
            if len(resp.content) > MAX_SIZE_BYTES:
                raise HTTPException(
                    status_code=413,
                    detail=f"Response too large: {len(resp.content)} bytes (max {MAX_SIZE_BYTES})"
                )
            
        except httpx.TimeoutException:
            raise HTTPException(status_code=504, detail="Upstream timeout")
        except httpx.HTTPStatusError as e:
            raise HTTPException(
                status_code=e.response.status_code,
                detail=f"Upstream error: {e.response.status_code}"
            )
        except httpx.RequestError as e:
            raise HTTPException(status_code=502, detail=f"Upstream request failed: {str(e)}")
    
    content_type = resp.headers.get("content-type", "application/pdf")
    
    filename = parsed.path.split("/")[-1] or "document.pdf"
    if not filename.lower().endswith(".pdf"):
        filename += ".pdf"
    
    return Response(
        content=resp.content,
        media_type=content_type,
        headers={
            "Content-Disposition": f'inline; filename="{filename}"',
            "Cache-Control": "public, max-age=3600",
            "X-Proxy-Source": hostname,
        }
    )


class NoCacheStaticFiles(StaticFiles):
    """Static files middleware with cache-control headers disabled."""
    async def get_response(self, path, scope):
        response = await super().get_response(path, scope)
        response.headers["Cache-Control"] = "no-store, no-cache, must-revalidate, max-age=0"
        response.headers["Pragma"] = "no-cache"
        response.headers["Expires"] = "0"
        return response


app.mount("/ui", NoCacheStaticFiles(directory=str(PROJECT_ROOT / "ui"), html=True), name="ui")
app.mount("/out", NoCacheStaticFiles(directory=str(PROJECT_ROOT / "out"), html=False), name="out")
app.mount("/config", NoCacheStaticFiles(directory=str(PROJECT_ROOT / "config"), html=False), name="config")
app.mount("/examples", NoCacheStaticFiles(directory=str(PROJECT_ROOT / "examples"), html=False), name="examples")
app.mount("/assets", NoCacheStaticFiles(directory=str(PROJECT_ROOT / "assets"), html=False), name="assets")
app.mount("/rules", NoCacheStaticFiles(directory=str(PROJECT_ROOT / "rules"), html=False), name="rules")


@app.get("/api/auth/config")
async def auth_config():
    """Return auth configuration for client-side Google Sign-In."""
    google_client_id = os.environ.get("GOOGLE_OAUTH_CLIENT_ID")
    return {
        "googleClientId": google_client_id,
        "configured": bool(google_client_id)
    }


@app.get("/api/pdf/text")
async def pdf_text_extract(url: str = Query(..., description="URL of the PDF to extract text from")):
    """
    Fetch a PDF and extract per-page text using PyMuPDF.
    Returns { pages: [{ page: 1, text: "..." }, ...] }
    Reuses the same security checks as /proxy/pdf.
    """
    try:
        decoded_url = unquote(url)
        parsed = urlparse(decoded_url)
    except Exception:
        raise HTTPException(status_code=400, detail="Invalid URL format")

    if parsed.scheme not in ("http", "https"):
        raise HTTPException(status_code=400, detail="Only HTTP/HTTPS URLs allowed")

    hostname = parsed.hostname
    if not hostname:
        raise HTTPException(status_code=400, detail="Missing hostname in URL")

    if not is_host_allowed(hostname):
        raise HTTPException(status_code=403, detail=f"Host not in allowlist: {hostname}")

    if is_private_ip(hostname):
        raise HTTPException(status_code=403, detail="Private/reserved IPs are blocked")

    async with httpx.AsyncClient(timeout=30.0, follow_redirects=False) as client:
        try:
            resp = await client.get(decoded_url)
            if resp.status_code in (301, 302, 303, 307, 308):
                redirect_url = resp.headers.get("location")
                if redirect_url:
                    redirect_parsed = urlparse(redirect_url)
                    redirect_host = redirect_parsed.hostname
                    if not redirect_host or not is_host_allowed(redirect_host) or is_private_ip(redirect_host):
                        raise HTTPException(status_code=403, detail="Redirect to non-allowlisted host blocked")
                    resp = await client.get(redirect_url)
            resp.raise_for_status()
            if len(resp.content) > MAX_SIZE_BYTES:
                raise HTTPException(status_code=413, detail="File too large")
        except httpx.TimeoutException:
            raise HTTPException(status_code=504, detail="Upstream timeout")
        except httpx.HTTPStatusError as e:
            raise HTTPException(status_code=e.response.status_code, detail=f"Upstream error: {e.response.status_code}")
        except httpx.RequestError as e:
            raise HTTPException(status_code=502, detail=f"Upstream request failed: {str(e)}")

    try:
        doc = fitz.open(stream=resp.content, filetype="pdf")
        pages = []
        for i in range(len(doc)):
            page = doc[i]
            text = page.get_text("text")
            pages.append({"page": i + 1, "text": text})
        doc.close()
    except Exception as e:
        raise HTTPException(status_code=422, detail=f"PDF text extraction failed: {str(e)}")

    return {"pages": pages, "total_pages": len(pages)}


@app.get("/")
async def root_redirect():
    """Redirect root to landing page."""
    from fastapi.responses import RedirectResponse
    return RedirectResponse(url="/ui/landing/index.html")


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=5000)
